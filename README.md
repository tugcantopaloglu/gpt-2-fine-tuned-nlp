# gpt-2-fine-tuned-nlp
This project focuses on fine-tuning the Generative Pre-trained Transformer 2 (GPT-2) model for a specialized task: answering mathematical questions in the Turkish language. Two different sizes of the model, `gpt2-medium` and `gpt2-large`, were fine-tuned on a custom Turkish Question-Answering (QA) dataset related to mathematics.
